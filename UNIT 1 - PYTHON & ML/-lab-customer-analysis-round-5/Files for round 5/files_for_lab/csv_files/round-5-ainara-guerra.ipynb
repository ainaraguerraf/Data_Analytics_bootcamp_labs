{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe613ba9",
   "metadata": {},
   "source": [
    "# Round 5 | Ainara Guerra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be532629",
   "metadata": {},
   "source": [
    "#### But first, libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.ticker as mk\n",
    "pd.set_option('display.max_columns', None)\n",
    "#%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f4c58",
   "metadata": {},
   "source": [
    "# FIRST PART: Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('marketing_customer_analysis.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179df1f",
   "metadata": {},
   "source": [
    "# SECOND PART: Dealing with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, columns: Standarize their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfe968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col_name.lower().replace(' ', '_') for col_name in df]\n",
    "df.columns = cols\n",
    "df = df.rename(columns={'employmentstatus': 'employment_status'})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc9eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's check the type of each columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea724c56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55264f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#what about nans??\n",
    "df.isna().sum()\n",
    "#looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ace65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store categoricals and numericals for later\n",
    "Z = df.copy()\n",
    "Z_num = Z.select_dtypes(include = np.number)\n",
    "Z_num_2 = Z_num.drop(['total_claim_amount'], axis=1)\n",
    "Z_cat = Z.select_dtypes(include = np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba26a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's explore a little bit more the categorical data to see if there's anything wrong\n",
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebda17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coverage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30384d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cc154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employment_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47989df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b03342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['policy_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['policy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c703f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['renew_offer_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['vehicle_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde952f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['vehicle_size'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b59f3c",
   "metadata": {},
   "source": [
    "# THIRD PART: EXPLORE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b168bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the response rate by sales channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cormac showed us this method and It's more efficient that what I did for my lab. \n",
    "x,y = 'response', 'sales_channel'\n",
    "\n",
    "df1 = df.groupby(x)[y].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\n",
    "g.ax.set_ylim(0,100)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(2)) + '%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e5a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A plot of the response rate by the total claim amount.\n",
    "#This is inspired by Nathi's work to try another type of exercise\n",
    "df2 = df.copy()\n",
    "bins = [0, 200, 400, 600, 1000, 3000]\n",
    "labels = ['0-200', '200-400', '400-600', '600-1000', '1000-3000']\n",
    "df2['binned'] = pd.cut(df2['total_claim_amount'], bins=bins, labels=labels) # Bin the \"total_claim_amount\" column into ranges\n",
    "df2['response'] = df2['response'].replace({'Yes': 1, 'No': 0}) # turn responses into numerical values\n",
    "grouped = df2.groupby('binned').agg({'response': 'sum', 'total_claim_amount': 'count'})\n",
    "grouped['response_rate'] = grouped['response'] / grouped['total_claim_amount'] # Calculate the response rate for each bin\n",
    "plt.bar(grouped.index, grouped['response_rate'], color = \"pink\")\n",
    "plt.xlabel('Total Claim Amount Range')\n",
    "plt.ylabel('Response Rate')\n",
    "plt.title('Response Rate by Total Claim Amount')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce577ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot of the response rate by the income.\n",
    "#This is also inspired by Nathi's work to try another type of exercise\n",
    "df3 = df.copy()\n",
    "bins2 = [0, 25000, 50000, 75000, 100000]\n",
    "labels2 = ['0-25000', '25000-50000', '50000-75000', '75000-100000']\n",
    "df3['binned'] = pd.cut(df3['income'], bins=bins2, labels=labels2) \n",
    "df3['response'] = df3['response'].replace({'Yes': 1, 'No': 0}) \n",
    "grouped1 = df3.groupby('binned').agg({'response': 'sum', 'income': 'count'}) \n",
    "grouped1['response_rate'] = grouped1['response'] / grouped1['income']\n",
    "plt.bar(grouped.index, grouped['response_rate'], color = \"purple\")\n",
    "plt.xlabel('Total Income Range')\n",
    "plt.ylabel('Response Rate')\n",
    "plt.title('Response Rate by Total Income Amount')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ca591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will try to check the normality of the numerical variables visually\n",
    "# Use seaborn library to construct distribution plots for the numerical variables\n",
    "\n",
    "for column in Z_num.columns:\n",
    "    sns.distplot(Z_num[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9c046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use Matplotlib to construct histograms\n",
    "fig, axs = plt.subplots(4, 2)\n",
    "fig.set_size_inches(8,8)\n",
    "axs[0, 0].hist(Z_num['customer_lifetime_value'])\n",
    "axs[0, 1].hist(Z_num['income'])\n",
    "axs[1, 0].hist(Z_num['monthly_premium_auto'])\n",
    "axs[1, 1].hist(Z_num['months_since_last_claim'])\n",
    "axs[2, 0].hist(Z_num[\"months_since_policy_inception\"])\n",
    "axs[2, 1].hist(Z_num[\"number_of_open_complaints\"])\n",
    "axs[3, 0].hist(Z_num[\"number_of_policies\"])\n",
    "axs[3, 1].hist(Z_num[\"total_claim_amount\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5af04",
   "metadata": {},
   "source": [
    "# FOURTH PART: Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655ece8",
   "metadata": {},
   "source": [
    "#### NORMALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e19c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the distributions for different numerical variables look like a normal distribution\n",
    "# we are not going to change anything in total claim amount, that's why we will use X_num_2\n",
    "\n",
    "transformer = MinMaxScaler().fit(Z_num_2)\n",
    "Z_minmax = transformer.transform(Z_num_2)\n",
    "Z_num_norm = pd.DataFrame(Z_minmax,columns=Z_num_2.columns)\n",
    "Z_num_norm.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185be75",
   "metadata": {},
   "source": [
    "#### CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable.\n",
    "sns.pairplot(Z_num)\n",
    "data_corr = Z_num.corr()\n",
    "data_corr = round(data_corr,2)\n",
    "data_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10a1dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop one of the two features that show a high correlation between them (greater than 0.9). \n",
    "#Write code for both the correlation matrix and for seaborn heatmap. \n",
    "#If there is no pair of features that have a high correlation, then do not drop any features\n",
    "#I appreciated that months since last claim, months since policy inception, \n",
    "#number of total complaints and number of policies are all highly correlated\n",
    "Z_num_3 = Z_num.drop(['months_since_policy_inception', 'number_of_policies', 'months_since_last_claim' ], axis=1)\n",
    "Z_num_3.head()\n",
    "data_corr_1 = Z_num_3.corr()\n",
    "data_corr_1 = round(data_corr_1,2)\n",
    "data_corr_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.heatmap(data_corr_1, annot=True)\n",
    "figure = sns_plot.get_figure()    \n",
    "figure.savefig('heatmap.png', dpi=400)\n",
    "mask = np.zeros_like(data_corr_1)\n",
    "mask[np.triu_indices_from(mask)] = True # optional, to hide repeat half of the matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(data_corr_1, mask=mask, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ccfedc",
   "metadata": {},
   "source": [
    "# X-Y SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['total_claim_amount']\n",
    "X = df.drop(['total_claim_amount'], axis=1)\n",
    "x_num = df.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(x_num,y) \n",
    "LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f57b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = lm.predict(x_num) \n",
    "rmse = mean_squared_error(y, predictions, squared=False)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "print(\"R2_score:\", round(lm.score(x_num,y),2)) \n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70010dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about if we use \"Z_num_3\" that we deleted the highly correlated values?\n",
    "Z_num_3_2 = Z_num_3.drop(['total_claim_amount'], axis=1)\n",
    "lm1 = LinearRegression()\n",
    "lm1.fit(Z_num_3_2,y) \n",
    "LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm1.predict(Z_num_3_2) \n",
    "rmse1 = mean_squared_error(y, predictions, squared=False)\n",
    "mae2 = mean_absolute_error(y, predictions)\n",
    "print(\"R2_score:\", round(lm1.score(Z_num_3_2,y),2)) \n",
    "print(\"RMSE:\", rmse1)\n",
    "print(\"MAE:\", mae2)\n",
    "\n",
    "#It is worse so we forget it about that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
